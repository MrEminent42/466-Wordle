{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQBviblF-FmH"
      },
      "outputs": [],
      "source": [
        "def debug(*info):\n",
        "    p = False\n",
        "    if p:\n",
        "        print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhCoexnK7Cih"
      },
      "source": [
        "# Wordle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxto3T2_7HMU",
        "outputId": "5fb6e91a-46d0-4e0e-ae2b-332370518bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: simple_colors in /usr/local/lib/python3.10/dist-packages (0.1.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install simple_colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8EBJOZz7CUT"
      },
      "outputs": [],
      "source": [
        "from enum import IntEnum\n",
        "from simple_colors import *\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "small_num_actions = 100\n",
        "\n",
        "actions = np.loadtxt(\"./actions.txt\", dtype=str)\n",
        "actions = random.sample([word.upper() for word in actions], small_num_actions)\n",
        "\n",
        "\n",
        "class Color(IntEnum):\n",
        "    GREY = GRAY = 0\n",
        "    YELLOW = 1\n",
        "    GREEN = 2\n",
        "\n",
        "\n",
        "class Tile:\n",
        "    def __init__(self, character: str, color: Color):\n",
        "        self.char = character\n",
        "        self.color = color\n",
        "\n",
        "\n",
        "class Board:\n",
        "    def __init__(self):\n",
        "        self.board = []\n",
        "\n",
        "    def append_row(self, row: list[Tile]):\n",
        "        self.board.append(row)\n",
        "\n",
        "    def get_row(self, i) -> list[Tile]:\n",
        "        return self.board[i]\n",
        "\n",
        "    def get_rows(self) -> list[list[Tile]]:\n",
        "        return self.board\n",
        "\n",
        "    def get_num_rows(self) -> int:\n",
        "        return len(self.board)\n",
        "\n",
        "\n",
        "class WordleGame:\n",
        "    def new_game(self):\n",
        "        self.answer = actions[np.random.randint(0, len(actions))]\n",
        "        self.board = Board()\n",
        "        self.is_complete = False\n",
        "        self.win = False\n",
        "\n",
        "    def __init__(self):\n",
        "        self.new_game()\n",
        "\n",
        "    ## string representation of the Wordle board\n",
        "    ## returns with color too!\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        # in each line\n",
        "        for i, line in enumerate(self.board.get_rows()):\n",
        "            colors = [tile.color for tile in line]\n",
        "            for i, tile in enumerate(line):\n",
        "                # for i, char in enumerate(line):\n",
        "\n",
        "                if tile.color == Color.GREY:\n",
        "                    s += black(tile.char, \"bold\")\n",
        "                elif tile.color == Color.YELLOW:\n",
        "                    s += yellow(tile.char, \"bold\")\n",
        "                else:\n",
        "                    s += green(tile.char, \"bold\")\n",
        "            s += \"\\n\"\n",
        "        return s\n",
        "\n",
        "    ## Takes a five-letter guess, records this guess on the game's board.\n",
        "    ## Returns the array of Colors with each index corresponding to the color of the letter at that index in the guess\n",
        "    def guess(self, guess):\n",
        "        debug(\"Guessing\", guess, \"on board\")\n",
        "        debug(self)\n",
        "        tiles = []\n",
        "        if len(guess) != 5:\n",
        "            raise ValueError(\n",
        "                'Wordle guess must be a 5-letter word. Could not guess with word \"'\n",
        "                + guess\n",
        "                + '\".'\n",
        "            )\n",
        "        # convert everything to upper case\n",
        "        guess = guess.upper()\n",
        "        # debug print\n",
        "        debug(\"Your guess:\", guess)\n",
        "        debug(\"The answer:\", self.answer)\n",
        "        colors = self.get_colors(guess)\n",
        "        # log guess to board\n",
        "        tiles = [Tile(guess[i], colors[i]) for i in range(5)]\n",
        "        self.board.append_row(tiles)\n",
        "\n",
        "        # check for game over\n",
        "        if self.board.get_num_rows() >= 6:\n",
        "            self.is_complete = True\n",
        "        elif guess == self.answer:\n",
        "            debug(\"WIN!\")\n",
        "            self.is_complete = self.win = True\n",
        "\n",
        "        # give back list of colors\n",
        "        return colors\n",
        "\n",
        "    ## get the colors of a word guess\n",
        "    ## input: string\n",
        "    ## output: list of Colors, each color corresponding to the\n",
        "    ##         appropriate game color of the letter at that index\n",
        "    def get_colors(self, guess: str):\n",
        "        ## grey by default\n",
        "        colors = [Color.GREY for i in range(len(guess))]\n",
        "\n",
        "        # count # occurrences of each of the letters in the correct answer\n",
        "        occurrences_remaining = {}\n",
        "        for char in self.answer:\n",
        "            if char in occurrences_remaining:\n",
        "                occurrences_remaining[char] += 1\n",
        "            else:\n",
        "                occurrences_remaining[char] = 1\n",
        "\n",
        "        ## appropriately color the letters\n",
        "\n",
        "        ## greens first\n",
        "        ## if the character is in the correct place\n",
        "        for i, char in enumerate(guess):\n",
        "            if self.answer[i] == char:\n",
        "                colors[i] = Color.GREEN\n",
        "                occurrences_remaining[char] -= 1\n",
        "                debug(\"Green:\", char)\n",
        "\n",
        "        ## yellows next\n",
        "        ## if the character is in the word, but in the wrong place\n",
        "        for i, char in enumerate(guess):\n",
        "            ## skip if already colored greeen\n",
        "            ## skip if all occurrences of this letter have been accounted for\n",
        "            if (\n",
        "                colors[i] == Color.GREEN\n",
        "                or char not in occurrences_remaining\n",
        "                or occurrences_remaining[char] == 0\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            colors[i] = Color.YELLOW\n",
        "            debug(\"Yellow:\", char)\n",
        "            # record that we have accounted for this occurence\n",
        "            occurrences_remaining[char] -= 1\n",
        "\n",
        "        return colors\n",
        "\n",
        "    def is_complete(self):\n",
        "        return self.is_complete\n",
        "\n",
        "    def run_game(self):\n",
        "        print(\"Welcome to Wordle-AI!\")\n",
        "        while not self.is_complete:\n",
        "            self.guess(input(\"Guess: \"))\n",
        "            print(self)\n",
        "        if self.win:\n",
        "            print(\n",
        "                \"Congrats! You found the word in\", self.board.get_num_rows(), \"tries.\"\n",
        "            )\n",
        "        else:\n",
        "            print(\"Darn! You didn't find the word. It was \" + self.answer + \".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whIrU-fh7VIL"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW_GgsRp7kTN"
      },
      "outputs": [],
      "source": [
        "# create the environment\n",
        "wordleGame = WordleGame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Par1IMmC7d5Q"
      },
      "source": [
        "## wordle wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIgXlQbc7fPT"
      },
      "outputs": [],
      "source": [
        "## convert a row on a board to one-hot encoded format for alphabet letters\n",
        "def rowToOneHot(row: list[str]):\n",
        "    oneHot = np.zeros((5, 26))\n",
        "    for i, char in enumerate(row):\n",
        "        if char != None:\n",
        "            oneHot[i][ord(char) - ord(\"A\")] = 1\n",
        "    return oneHot\n",
        "\n",
        "\n",
        "## convert a board to a state, which is a flattened version of:\n",
        "## the board: (6 x 5) wordle board x 26 letters one-hot encoded\n",
        "## +  colors: (6 x 5) wordle board x 2 color layers (green and yellow,\n",
        "##                                                  grey is default)\n",
        "def boardToState(board: Board):\n",
        "    letters = np.zeros((6, 5, 26))\n",
        "    colors = np.zeros((6, 5, 2))\n",
        "    # 840 total size of board state\n",
        "\n",
        "    for i, row in enumerate(board.get_rows()):\n",
        "        for j, tile in enumerate(row):\n",
        "            letters[i][j][ord(tile.char) - ord(\"A\")] = 1\n",
        "            if tile.color == Color.GREEN:\n",
        "                colors[i][j][1] = 1\n",
        "            elif tile.color == Color.YELLOW:\n",
        "                colors[i][j][0] = 1\n",
        "\n",
        "    return np.concatenate((letters.flatten(), colors.flatten()))\n",
        "\n",
        "\n",
        "## get the reward of a guess based on the colors\n",
        "def getReward(colors: list[Color]):\n",
        "    reward = 0\n",
        "    win = True\n",
        "    for color in colors:\n",
        "        if color == Color.GREEN:\n",
        "            reward += 1.5\n",
        "        elif color == Color.YELLOW:\n",
        "            reward += 1.0\n",
        "            win = False\n",
        "        else:\n",
        "            reward += 0.5\n",
        "            win = False\n",
        "    if win:\n",
        "        reward += 5\n",
        "    return reward\n",
        "\n",
        "\n",
        "## convert an action index to model input format\n",
        "def actionIndToInput(action_ind: int):\n",
        "    action = actions[action_ind]\n",
        "    word = []\n",
        "    for letter in action:\n",
        "        tile = [0 for i in range(26)]\n",
        "        tile[ord(letter) - ord(\"A\")] = 1\n",
        "        word.append(tile)\n",
        "    return np.concatenate(np.array(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4heCIcq0QldL"
      },
      "outputs": [],
      "source": [
        "def inputToLegible(input):\n",
        "    # letters are first 6 x 5 x 26  = 780\n",
        "    arr = np.reshape(input[0:780], (6, 5, 26))\n",
        "    board = [[\"\" for i in range(5)] for i in range(6)]\n",
        "    for row in range(6):\n",
        "        for col in range(5):\n",
        "            for offset in range(26):\n",
        "                if arr[row][col][offset] == 1:\n",
        "                    board[row][col] = chr(offset + ord(\"A\"))\n",
        "        board[row] = \"\".join(board[row])\n",
        "\n",
        "    guess_arr = np.reshape(input[-130:], (5, 26))\n",
        "    word = [\"\" for i in range(5)]\n",
        "    for tile in range(5):\n",
        "        for offset in range(26):\n",
        "            if guess_arr[tile][offset] == 1:\n",
        "                word[tile] = chr(offset + ord(\"A\"))\n",
        "\n",
        "    return (board, \"\".join(word))\n",
        "\n",
        "\n",
        "# inputToLegible(np.concatenate((boardToState(wordleGame.board), actionIndToInput(100))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv2rl4yj7xFL"
      },
      "outputs": [],
      "source": [
        "## step function for the environment\n",
        "## input: int action for the word to guess\n",
        "## output: vector next_state, float reward, boolean done, None info\n",
        "def step(action: int):\n",
        "    ## verify action is legit\n",
        "    if action < 0 or action >= len(actions):\n",
        "        raise ValueError(\"Action out of bounds\")\n",
        "\n",
        "    ## take the action (guess)\n",
        "    guess = actions[action]\n",
        "    colors = wordleGame.guess(guess)\n",
        "    ## what we need: next_state, reward, done, info (not used)\n",
        "    reward = getReward(colors)\n",
        "    next_state = boardToState(wordleGame.board)\n",
        "    done = wordleGame.is_complete\n",
        "    return next_state, reward, done, None\n",
        "\n",
        "\n",
        "def reset():\n",
        "    debug(\"Resetting game\")\n",
        "    wordleGame.new_game()\n",
        "    return boardToState(wordleGame.board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeC046m7r3T"
      },
      "source": [
        "## replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0odSj0O72cn"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done: bool):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    ## random sample of the buffer crossed with a set of random actions of num_actions\n",
        "    ## input: int num_samples, int num_actions\n",
        "    ## output: tuple of np.arrays (states, actions, rewards, next_states, dones, potential_actions)\n",
        "    ## of length num_samples * num_actions\n",
        "    def sample(self, num_samples, num_actions):\n",
        "        states, selected_actions, rewards, next_states, dones, potential_actions = (\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "        )\n",
        "        indexes = np.random.choice(len(self.buffer), num_samples)\n",
        "\n",
        "        potential_action_indices = np.random.choice(len(actions), num_actions)\n",
        "\n",
        "        for i in indexes:\n",
        "            for potential_action_index in potential_action_indices:\n",
        "                state, action, reward, next_state, done = self.buffer[i]\n",
        "                states.append(state)\n",
        "                selected_actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                next_states.append(next_state)\n",
        "                dones.append(done)\n",
        "                potential_actions.append(actionIndToInput(potential_action_index))\n",
        "\n",
        "        return (\n",
        "            np.array(states),  # vector array\n",
        "            np.array(actions),  # int array\n",
        "            np.array(rewards),  # float array\n",
        "            np.array(next_states),  # vector array\n",
        "            np.array(dones, dtype=float),  # vector array, 1.0 = true, 0.0 = false\n",
        "            np.array(potential_actions),  # int array\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsm4U9lb75E9"
      },
      "source": [
        "## models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7npGrnBUCls"
      },
      "outputs": [],
      "source": [
        "## model input size:\n",
        "## wordle letters:             6*5*26\n",
        "## colors for each tile:     + 6*5*2\n",
        "## action to predict reward: + 5*26\n",
        "##                           = 970"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj4DNoso8BOz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enPGTHm37576"
      },
      "outputs": [],
      "source": [
        "model_q = tf.keras.models.Sequential(\n",
        "    [\n",
        "        # tf.keras.layers.Flatten(input_shape=(840,)),  # 840 inputs\n",
        "        tf.keras.layers.Dense(970, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_qp = tf.keras.models.Sequential(\n",
        "    [\n",
        "        # tf.keras.layers.Flatten(input_shape=(840,)),  # 840 inputs\n",
        "        tf.keras.layers.Dense(970, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3BGvLlG8Hby"
      },
      "source": [
        "## training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEJSFntfXd0_"
      },
      "outputs": [],
      "source": [
        "def get_model_input(state, action_ind):\n",
        "    return np.concatenate((boardToState(state), actionIndToInput(action_ind)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D8BEtAQm8iR"
      },
      "outputs": [],
      "source": [
        "# select_epsilon_greedy_action moved below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq0gb4g_8LLe"
      },
      "outputs": [],
      "source": [
        "num_episodes = 1000  # @param {type: \"integer\"}\n",
        "epsilon = 1.0  # @param {type: \"number\"}\n",
        "batch_size = 32  # @param {type: \"integer\"}\n",
        "action_size = 32  # @param {type: \"integer\"}\n",
        "discount = 0.99  # @param {type: \"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPQtyL6-agZD"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9P8deCH8hgw",
        "outputId": "afff9287-dcda-4ebe-ada3-f8ab7a7caeb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0/1000. Epsilon: 0.990. Reward in last 100 episodes: 21.000\n",
            "Episode 2/1000. Epsilon: 0.970. Reward in last 100 episodes: 20.833\n",
            "Episode 4/1000. Epsilon: 0.950. Reward in last 100 episodes: 20.900\n",
            "Episode 6/1000. Epsilon: 0.930. Reward in last 100 episodes: 20.643\n",
            "Episode 8/1000. Epsilon: 0.910. Reward in last 100 episodes: 20.889\n",
            "Episode 10/1000. Epsilon: 0.890. Reward in last 100 episodes: 20.455\n",
            "Episode 12/1000. Epsilon: 0.870. Reward in last 100 episodes: 20.308\n",
            "Episode 14/1000. Epsilon: 0.850. Reward in last 100 episodes: 20.333\n",
            "Episode 16/1000. Epsilon: 0.830. Reward in last 100 episodes: 19.971\n",
            "Episode 18/1000. Epsilon: 0.810. Reward in last 100 episodes: 20.132\n",
            "Episode 20/1000. Epsilon: 0.790. Reward in last 100 episodes: 20.071\n",
            "Episode 22/1000. Epsilon: 0.770. Reward in last 100 episodes: 19.848\n",
            "Episode 24/1000. Epsilon: 0.750. Reward in last 100 episodes: 19.920\n",
            "Episode 26/1000. Epsilon: 0.730. Reward in last 100 episodes: 19.852\n",
            "Episode 28/1000. Epsilon: 0.710. Reward in last 100 episodes: 19.931\n",
            "Episode 30/1000. Epsilon: 0.690. Reward in last 100 episodes: 19.871\n",
            "Episode 32/1000. Epsilon: 0.670. Reward in last 100 episodes: 19.803\n",
            "Episode 34/1000. Epsilon: 0.650. Reward in last 100 episodes: 19.843\n",
            "Episode 36/1000. Epsilon: 0.630. Reward in last 100 episodes: 19.946\n",
            "Episode 38/1000. Epsilon: 0.610. Reward in last 100 episodes: 19.923\n",
            "Episode 40/1000. Epsilon: 0.590. Reward in last 100 episodes: 19.817\n",
            "Episode 42/1000. Epsilon: 0.570. Reward in last 100 episodes: 19.640\n",
            "Episode 44/1000. Epsilon: 0.550. Reward in last 100 episodes: 19.644\n",
            "Episode 46/1000. Epsilon: 0.530. Reward in last 100 episodes: 19.574\n",
            "Episode 48/1000. Epsilon: 0.510. Reward in last 100 episodes: 19.622\n",
            "Episode 50/1000. Epsilon: 0.490. Reward in last 100 episodes: 19.647\n",
            "Episode 52/1000. Epsilon: 0.470. Reward in last 100 episodes: 19.783\n",
            "Episode 54/1000. Epsilon: 0.450. Reward in last 100 episodes: 19.845\n",
            "Episode 56/1000. Epsilon: 0.430. Reward in last 100 episodes: 19.789\n",
            "Episode 58/1000. Epsilon: 0.410. Reward in last 100 episodes: 19.932\n",
            "Episode 60/1000. Epsilon: 0.390. Reward in last 100 episodes: 19.934\n",
            "Episode 62/1000. Epsilon: 0.370. Reward in last 100 episodes: 19.929\n",
            "Episode 64/1000. Epsilon: 0.350. Reward in last 100 episodes: 19.915\n",
            "Episode 66/1000. Epsilon: 0.330. Reward in last 100 episodes: 19.873\n",
            "Episode 68/1000. Epsilon: 0.310. Reward in last 100 episodes: 19.899\n",
            "Episode 70/1000. Epsilon: 0.290. Reward in last 100 episodes: 19.944\n",
            "Episode 72/1000. Epsilon: 0.270. Reward in last 100 episodes: 19.932\n",
            "Episode 74/1000. Epsilon: 0.250. Reward in last 100 episodes: 19.880\n",
            "Episode 76/1000. Epsilon: 0.230. Reward in last 100 episodes: 19.877\n",
            "Episode 78/1000. Epsilon: 0.210. Reward in last 100 episodes: 19.911\n",
            "Episode 80/1000. Epsilon: 0.190. Reward in last 100 episodes: 19.920\n",
            "Episode 82/1000. Epsilon: 0.170. Reward in last 100 episodes: 20.090\n",
            "Episode 84/1000. Epsilon: 0.150. Reward in last 100 episodes: 20.041\n",
            "Episode 86/1000. Epsilon: 0.130. Reward in last 100 episodes: 20.052\n",
            "Episode 88/1000. Epsilon: 0.110. Reward in last 100 episodes: 20.000\n",
            "Episode 90/1000. Epsilon: 0.090. Reward in last 100 episodes: 20.132\n",
            "Episode 92/1000. Epsilon: 0.070. Reward in last 100 episodes: 20.339\n",
            "Episode 94/1000. Epsilon: 0.050. Reward in last 100 episodes: 20.384\n",
            "Episode 96/1000. Epsilon: 0.030. Reward in last 100 episodes: 20.448\n",
            "Episode 98/1000. Epsilon: 0.010. Reward in last 100 episodes: 20.399\n",
            "Episode 100/1000. Epsilon: -0.010. Reward in last 100 episodes: 20.345\n",
            "Episode 102/1000. Epsilon: -0.030. Reward in last 100 episodes: 20.500\n",
            "Episode 104/1000. Epsilon: -0.050. Reward in last 100 episodes: 20.560\n",
            "Episode 106/1000. Epsilon: -0.070. Reward in last 100 episodes: 20.495\n",
            "Episode 108/1000. Epsilon: -0.090. Reward in last 100 episodes: 20.600\n",
            "Episode 110/1000. Epsilon: -0.110. Reward in last 100 episodes: 20.590\n",
            "Episode 112/1000. Epsilon: -0.130. Reward in last 100 episodes: 20.620\n",
            "Episode 114/1000. Epsilon: -0.150. Reward in last 100 episodes: 20.660\n",
            "Episode 116/1000. Epsilon: -0.170. Reward in last 100 episodes: 20.675\n",
            "Episode 118/1000. Epsilon: -0.190. Reward in last 100 episodes: 20.755\n",
            "Episode 120/1000. Epsilon: -0.210. Reward in last 100 episodes: 20.905\n",
            "Episode 122/1000. Epsilon: -0.230. Reward in last 100 episodes: 21.035\n",
            "Episode 124/1000. Epsilon: -0.250. Reward in last 100 episodes: 21.010\n",
            "Episode 126/1000. Epsilon: -0.270. Reward in last 100 episodes: 21.080\n",
            "Episode 128/1000. Epsilon: -0.290. Reward in last 100 episodes: 21.080\n",
            "Episode 130/1000. Epsilon: -0.310. Reward in last 100 episodes: 21.270\n",
            "Episode 132/1000. Epsilon: -0.330. Reward in last 100 episodes: 21.255\n",
            "Episode 134/1000. Epsilon: -0.350. Reward in last 100 episodes: 21.175\n",
            "Episode 136/1000. Epsilon: -0.370. Reward in last 100 episodes: 21.160\n",
            "Episode 138/1000. Epsilon: -0.390. Reward in last 100 episodes: 21.130\n",
            "Episode 140/1000. Epsilon: -0.410. Reward in last 100 episodes: 21.135\n",
            "Episode 142/1000. Epsilon: -0.430. Reward in last 100 episodes: 21.295\n",
            "Episode 144/1000. Epsilon: -0.450. Reward in last 100 episodes: 21.450\n",
            "Episode 146/1000. Epsilon: -0.470. Reward in last 100 episodes: 21.420\n",
            "Episode 148/1000. Epsilon: -0.490. Reward in last 100 episodes: 21.420\n",
            "Episode 150/1000. Epsilon: -0.510. Reward in last 100 episodes: 21.435\n",
            "Episode 152/1000. Epsilon: -0.530. Reward in last 100 episodes: 21.390\n",
            "Episode 154/1000. Epsilon: -0.550. Reward in last 100 episodes: 21.530\n",
            "Episode 156/1000. Epsilon: -0.570. Reward in last 100 episodes: 21.695\n",
            "Episode 158/1000. Epsilon: -0.590. Reward in last 100 episodes: 21.575\n",
            "Episode 160/1000. Epsilon: -0.610. Reward in last 100 episodes: 21.625\n",
            "Episode 162/1000. Epsilon: -0.630. Reward in last 100 episodes: 21.680\n",
            "Episode 164/1000. Epsilon: -0.650. Reward in last 100 episodes: 21.760\n",
            "Episode 166/1000. Epsilon: -0.670. Reward in last 100 episodes: 21.870\n",
            "Episode 168/1000. Epsilon: -0.690. Reward in last 100 episodes: 21.965\n",
            "Episode 170/1000. Epsilon: -0.710. Reward in last 100 episodes: 21.925\n",
            "Episode 172/1000. Epsilon: -0.730. Reward in last 100 episodes: 21.835\n",
            "Episode 174/1000. Epsilon: -0.750. Reward in last 100 episodes: 21.920\n",
            "Episode 176/1000. Epsilon: -0.770. Reward in last 100 episodes: 21.965\n",
            "Episode 178/1000. Epsilon: -0.790. Reward in last 100 episodes: 22.080\n",
            "Episode 180/1000. Epsilon: -0.810. Reward in last 100 episodes: 22.245\n",
            "Episode 182/1000. Epsilon: -0.830. Reward in last 100 episodes: 22.275\n",
            "Episode 184/1000. Epsilon: -0.850. Reward in last 100 episodes: 22.335\n",
            "Episode 186/1000. Epsilon: -0.870. Reward in last 100 episodes: 22.405\n",
            "Episode 188/1000. Epsilon: -0.890. Reward in last 100 episodes: 22.500\n",
            "Episode 190/1000. Epsilon: -0.910. Reward in last 100 episodes: 22.420\n",
            "Episode 192/1000. Epsilon: -0.930. Reward in last 100 episodes: 22.335\n",
            "Episode 194/1000. Epsilon: -0.950. Reward in last 100 episodes: 22.245\n",
            "Episode 196/1000. Epsilon: -0.970. Reward in last 100 episodes: 22.195\n",
            "Episode 198/1000. Epsilon: -0.990. Reward in last 100 episodes: 22.345\n",
            "Episode 200/1000. Epsilon: -1.010. Reward in last 100 episodes: 22.345\n",
            "Episode 202/1000. Epsilon: -1.030. Reward in last 100 episodes: 22.135\n",
            "Episode 204/1000. Epsilon: -1.050. Reward in last 100 episodes: 22.020\n",
            "Episode 206/1000. Epsilon: -1.070. Reward in last 100 episodes: 22.225\n",
            "Episode 208/1000. Epsilon: -1.090. Reward in last 100 episodes: 22.150\n",
            "Episode 210/1000. Epsilon: -1.110. Reward in last 100 episodes: 22.160\n",
            "Episode 212/1000. Epsilon: -1.130. Reward in last 100 episodes: 22.040\n",
            "Episode 214/1000. Epsilon: -1.150. Reward in last 100 episodes: 21.965\n",
            "Episode 216/1000. Epsilon: -1.170. Reward in last 100 episodes: 22.080\n",
            "Episode 218/1000. Epsilon: -1.190. Reward in last 100 episodes: 22.020\n",
            "Episode 220/1000. Epsilon: -1.210. Reward in last 100 episodes: 21.990\n",
            "Episode 222/1000. Epsilon: -1.230. Reward in last 100 episodes: 21.900\n",
            "Episode 224/1000. Epsilon: -1.250. Reward in last 100 episodes: 21.960\n",
            "Episode 226/1000. Epsilon: -1.270. Reward in last 100 episodes: 21.990\n",
            "Episode 228/1000. Epsilon: -1.290. Reward in last 100 episodes: 22.055\n",
            "Episode 230/1000. Epsilon: -1.310. Reward in last 100 episodes: 21.990\n",
            "Episode 232/1000. Epsilon: -1.330. Reward in last 100 episodes: 21.990\n",
            "Episode 234/1000. Epsilon: -1.350. Reward in last 100 episodes: 22.105\n",
            "Episode 236/1000. Epsilon: -1.370. Reward in last 100 episodes: 22.105\n",
            "Episode 238/1000. Epsilon: -1.390. Reward in last 100 episodes: 22.225\n",
            "Episode 240/1000. Epsilon: -1.410. Reward in last 100 episodes: 22.440\n",
            "Episode 242/1000. Epsilon: -1.430. Reward in last 100 episodes: 22.380\n",
            "Episode 244/1000. Epsilon: -1.450. Reward in last 100 episodes: 22.280\n",
            "Episode 246/1000. Epsilon: -1.470. Reward in last 100 episodes: 22.495\n",
            "Episode 248/1000. Epsilon: -1.490. Reward in last 100 episodes: 22.485\n",
            "Episode 250/1000. Epsilon: -1.510. Reward in last 100 episodes: 22.415\n",
            "Episode 252/1000. Epsilon: -1.530. Reward in last 100 episodes: 22.410\n",
            "Episode 254/1000. Epsilon: -1.550. Reward in last 100 episodes: 22.350\n",
            "Episode 256/1000. Epsilon: -1.570. Reward in last 100 episodes: 22.265\n",
            "Episode 258/1000. Epsilon: -1.590. Reward in last 100 episodes: 22.270\n",
            "Episode 260/1000. Epsilon: -1.610. Reward in last 100 episodes: 22.280\n",
            "Episode 262/1000. Epsilon: -1.630. Reward in last 100 episodes: 22.310\n",
            "Episode 264/1000. Epsilon: -1.650. Reward in last 100 episodes: 22.360\n",
            "Episode 266/1000. Epsilon: -1.670. Reward in last 100 episodes: 22.410\n",
            "Episode 268/1000. Epsilon: -1.690. Reward in last 100 episodes: 22.430\n",
            "Episode 270/1000. Epsilon: -1.710. Reward in last 100 episodes: 22.520\n",
            "Episode 272/1000. Epsilon: -1.730. Reward in last 100 episodes: 22.610\n",
            "Episode 274/1000. Epsilon: -1.750. Reward in last 100 episodes: 22.765\n",
            "Episode 276/1000. Epsilon: -1.770. Reward in last 100 episodes: 22.775\n",
            "Episode 278/1000. Epsilon: -1.790. Reward in last 100 episodes: 22.710\n",
            "Episode 280/1000. Epsilon: -1.810. Reward in last 100 episodes: 22.680\n",
            "Episode 282/1000. Epsilon: -1.830. Reward in last 100 episodes: 22.470\n",
            "Episode 284/1000. Epsilon: -1.850. Reward in last 100 episodes: 22.380\n",
            "Episode 286/1000. Epsilon: -1.870. Reward in last 100 episodes: 22.270\n",
            "Episode 288/1000. Epsilon: -1.890. Reward in last 100 episodes: 22.120\n",
            "Episode 290/1000. Epsilon: -1.910. Reward in last 100 episodes: 22.025\n",
            "Episode 292/1000. Epsilon: -1.930. Reward in last 100 episodes: 21.940\n",
            "Episode 294/1000. Epsilon: -1.950. Reward in last 100 episodes: 21.955\n",
            "Episode 296/1000. Epsilon: -1.970. Reward in last 100 episodes: 21.865\n",
            "Episode 298/1000. Epsilon: -1.990. Reward in last 100 episodes: 21.750\n",
            "Episode 300/1000. Epsilon: -2.010. Reward in last 100 episodes: 21.780\n",
            "Episode 302/1000. Epsilon: -2.030. Reward in last 100 episodes: 21.825\n",
            "Episode 304/1000. Epsilon: -2.050. Reward in last 100 episodes: 21.905\n",
            "Episode 306/1000. Epsilon: -2.070. Reward in last 100 episodes: 21.665\n",
            "Episode 308/1000. Epsilon: -2.090. Reward in last 100 episodes: 21.620\n",
            "Episode 310/1000. Epsilon: -2.110. Reward in last 100 episodes: 21.670\n",
            "Episode 312/1000. Epsilon: -2.130. Reward in last 100 episodes: 21.790\n",
            "Episode 314/1000. Epsilon: -2.150. Reward in last 100 episodes: 21.775\n",
            "Episode 316/1000. Epsilon: -2.170. Reward in last 100 episodes: 21.820\n",
            "Episode 318/1000. Epsilon: -2.190. Reward in last 100 episodes: 21.820\n",
            "Episode 320/1000. Epsilon: -2.210. Reward in last 100 episodes: 21.770\n",
            "Episode 322/1000. Epsilon: -2.230. Reward in last 100 episodes: 21.915\n",
            "Episode 324/1000. Epsilon: -2.250. Reward in last 100 episodes: 21.770\n",
            "Episode 326/1000. Epsilon: -2.270. Reward in last 100 episodes: 21.705\n",
            "Episode 328/1000. Epsilon: -2.290. Reward in last 100 episodes: 21.520\n",
            "Episode 330/1000. Epsilon: -2.310. Reward in last 100 episodes: 21.505\n",
            "Episode 332/1000. Epsilon: -2.330. Reward in last 100 episodes: 21.715\n",
            "Episode 334/1000. Epsilon: -2.350. Reward in last 100 episodes: 21.725\n",
            "Episode 336/1000. Epsilon: -2.370. Reward in last 100 episodes: 21.785\n",
            "Episode 338/1000. Epsilon: -2.390. Reward in last 100 episodes: 21.720\n",
            "Episode 340/1000. Epsilon: -2.410. Reward in last 100 episodes: 21.510\n",
            "Episode 342/1000. Epsilon: -2.430. Reward in last 100 episodes: 21.495\n",
            "Episode 344/1000. Epsilon: -2.450. Reward in last 100 episodes: 21.440\n",
            "Episode 346/1000. Epsilon: -2.470. Reward in last 100 episodes: 21.290\n",
            "Episode 348/1000. Epsilon: -2.490. Reward in last 100 episodes: 21.350\n",
            "Episode 350/1000. Epsilon: -2.510. Reward in last 100 episodes: 21.575\n",
            "Episode 352/1000. Epsilon: -2.530. Reward in last 100 episodes: 21.605\n",
            "Episode 354/1000. Epsilon: -2.550. Reward in last 100 episodes: 21.600\n",
            "Episode 356/1000. Epsilon: -2.570. Reward in last 100 episodes: 21.570\n",
            "Episode 358/1000. Epsilon: -2.590. Reward in last 100 episodes: 21.640\n",
            "Episode 360/1000. Epsilon: -2.610. Reward in last 100 episodes: 21.820\n",
            "Episode 362/1000. Epsilon: -2.630. Reward in last 100 episodes: 21.690\n",
            "Episode 364/1000. Epsilon: -2.650. Reward in last 100 episodes: 21.730\n",
            "Episode 366/1000. Epsilon: -2.670. Reward in last 100 episodes: 21.610\n",
            "Episode 368/1000. Epsilon: -2.690. Reward in last 100 episodes: 21.520\n",
            "Episode 370/1000. Epsilon: -2.710. Reward in last 100 episodes: 21.325\n",
            "Episode 372/1000. Epsilon: -2.730. Reward in last 100 episodes: 21.330\n",
            "Episode 374/1000. Epsilon: -2.750. Reward in last 100 episodes: 21.130\n",
            "Episode 376/1000. Epsilon: -2.770. Reward in last 100 episodes: 21.160\n",
            "Episode 378/1000. Epsilon: -2.790. Reward in last 100 episodes: 21.140\n",
            "Episode 380/1000. Epsilon: -2.810. Reward in last 100 episodes: 21.000\n",
            "Episode 382/1000. Epsilon: -2.830. Reward in last 100 episodes: 21.055\n",
            "Episode 384/1000. Epsilon: -2.850. Reward in last 100 episodes: 21.090\n",
            "Episode 386/1000. Epsilon: -2.870. Reward in last 100 episodes: 21.130\n",
            "Episode 388/1000. Epsilon: -2.890. Reward in last 100 episodes: 21.235\n",
            "Episode 390/1000. Epsilon: -2.910. Reward in last 100 episodes: 21.300\n",
            "Episode 392/1000. Epsilon: -2.930. Reward in last 100 episodes: 21.240\n",
            "Episode 394/1000. Epsilon: -2.950. Reward in last 100 episodes: 21.245\n",
            "Episode 396/1000. Epsilon: -2.970. Reward in last 100 episodes: 21.325\n",
            "Episode 398/1000. Epsilon: -2.990. Reward in last 100 episodes: 21.395\n",
            "Episode 400/1000. Epsilon: -3.010. Reward in last 100 episodes: 21.385\n",
            "Episode 402/1000. Epsilon: -3.030. Reward in last 100 episodes: 21.390\n",
            "Episode 404/1000. Epsilon: -3.050. Reward in last 100 episodes: 21.315\n",
            "Episode 406/1000. Epsilon: -3.070. Reward in last 100 episodes: 21.360\n",
            "Episode 408/1000. Epsilon: -3.090. Reward in last 100 episodes: 21.480\n",
            "Episode 410/1000. Epsilon: -3.110. Reward in last 100 episodes: 21.475\n",
            "Episode 412/1000. Epsilon: -3.130. Reward in last 100 episodes: 21.520\n",
            "Episode 414/1000. Epsilon: -3.150. Reward in last 100 episodes: 21.650\n",
            "Episode 416/1000. Epsilon: -3.170. Reward in last 100 episodes: 21.550\n",
            "Episode 418/1000. Epsilon: -3.190. Reward in last 100 episodes: 21.405\n",
            "Episode 420/1000. Epsilon: -3.210. Reward in last 100 episodes: 21.265\n",
            "Episode 422/1000. Epsilon: -3.230. Reward in last 100 episodes: 21.130\n",
            "Episode 424/1000. Epsilon: -3.250. Reward in last 100 episodes: 21.215\n",
            "Episode 426/1000. Epsilon: -3.270. Reward in last 100 episodes: 21.100\n",
            "Episode 428/1000. Epsilon: -3.290. Reward in last 100 episodes: 21.190\n",
            "Episode 430/1000. Epsilon: -3.310. Reward in last 100 episodes: 21.085\n",
            "Episode 432/1000. Epsilon: -3.330. Reward in last 100 episodes: 20.860\n",
            "Episode 434/1000. Epsilon: -3.350. Reward in last 100 episodes: 20.860\n",
            "Episode 436/1000. Epsilon: -3.370. Reward in last 100 episodes: 20.745\n",
            "Episode 438/1000. Epsilon: -3.390. Reward in last 100 episodes: 20.640\n",
            "Episode 440/1000. Epsilon: -3.410. Reward in last 100 episodes: 20.610\n",
            "Episode 442/1000. Epsilon: -3.430. Reward in last 100 episodes: 20.690\n",
            "Episode 444/1000. Epsilon: -3.450. Reward in last 100 episodes: 20.875\n",
            "Episode 446/1000. Epsilon: -3.470. Reward in last 100 episodes: 20.900\n",
            "Episode 448/1000. Epsilon: -3.490. Reward in last 100 episodes: 20.885\n",
            "Episode 450/1000. Epsilon: -3.510. Reward in last 100 episodes: 20.670\n",
            "Episode 452/1000. Epsilon: -3.530. Reward in last 100 episodes: 20.765\n",
            "Episode 454/1000. Epsilon: -3.550. Reward in last 100 episodes: 20.560\n",
            "Episode 456/1000. Epsilon: -3.570. Reward in last 100 episodes: 20.655\n",
            "Episode 458/1000. Epsilon: -3.590. Reward in last 100 episodes: 20.555\n",
            "Episode 460/1000. Epsilon: -3.610. Reward in last 100 episodes: 20.360\n",
            "Episode 462/1000. Epsilon: -3.630. Reward in last 100 episodes: 20.490\n",
            "Episode 464/1000. Epsilon: -3.650. Reward in last 100 episodes: 20.320\n",
            "Episode 466/1000. Epsilon: -3.670. Reward in last 100 episodes: 20.330\n",
            "Episode 468/1000. Epsilon: -3.690. Reward in last 100 episodes: 20.310\n",
            "Episode 470/1000. Epsilon: -3.710. Reward in last 100 episodes: 20.385\n",
            "Episode 472/1000. Epsilon: -3.730. Reward in last 100 episodes: 20.470\n",
            "Episode 474/1000. Epsilon: -3.750. Reward in last 100 episodes: 20.580\n",
            "Episode 476/1000. Epsilon: -3.770. Reward in last 100 episodes: 20.595\n",
            "Episode 478/1000. Epsilon: -3.790. Reward in last 100 episodes: 20.470\n",
            "Episode 480/1000. Epsilon: -3.810. Reward in last 100 episodes: 20.465\n",
            "Episode 482/1000. Epsilon: -3.830. Reward in last 100 episodes: 20.415\n",
            "Episode 484/1000. Epsilon: -3.850. Reward in last 100 episodes: 20.380\n",
            "Episode 486/1000. Epsilon: -3.870. Reward in last 100 episodes: 20.480\n",
            "Episode 488/1000. Epsilon: -3.890. Reward in last 100 episodes: 20.550\n",
            "Episode 490/1000. Epsilon: -3.910. Reward in last 100 episodes: 20.565\n",
            "Episode 492/1000. Epsilon: -3.930. Reward in last 100 episodes: 20.630\n",
            "Episode 494/1000. Epsilon: -3.950. Reward in last 100 episodes: 20.640\n",
            "Episode 496/1000. Epsilon: -3.970. Reward in last 100 episodes: 20.590\n",
            "Episode 498/1000. Epsilon: -3.990. Reward in last 100 episodes: 20.530\n",
            "Episode 500/1000. Epsilon: -4.010. Reward in last 100 episodes: 20.545\n",
            "Episode 502/1000. Epsilon: -4.030. Reward in last 100 episodes: 20.485\n",
            "Episode 504/1000. Epsilon: -4.050. Reward in last 100 episodes: 20.590\n",
            "Episode 506/1000. Epsilon: -4.070. Reward in last 100 episodes: 20.595\n",
            "Episode 508/1000. Epsilon: -4.090. Reward in last 100 episodes: 20.500\n",
            "Episode 510/1000. Epsilon: -4.110. Reward in last 100 episodes: 20.535\n",
            "Episode 512/1000. Epsilon: -4.130. Reward in last 100 episodes: 20.520\n",
            "Episode 514/1000. Epsilon: -4.150. Reward in last 100 episodes: 20.415\n",
            "Episode 516/1000. Epsilon: -4.170. Reward in last 100 episodes: 20.455\n",
            "Episode 518/1000. Epsilon: -4.190. Reward in last 100 episodes: 20.580\n",
            "Episode 520/1000. Epsilon: -4.210. Reward in last 100 episodes: 20.620\n",
            "Episode 522/1000. Epsilon: -4.230. Reward in last 100 episodes: 20.700\n",
            "Episode 524/1000. Epsilon: -4.250. Reward in last 100 episodes: 20.645\n",
            "Episode 526/1000. Epsilon: -4.270. Reward in last 100 episodes: 20.745\n",
            "Episode 528/1000. Epsilon: -4.290. Reward in last 100 episodes: 20.730\n",
            "Episode 530/1000. Epsilon: -4.310. Reward in last 100 episodes: 20.775\n",
            "Episode 532/1000. Epsilon: -4.330. Reward in last 100 episodes: 20.785\n",
            "Episode 534/1000. Epsilon: -4.350. Reward in last 100 episodes: 20.725\n",
            "Episode 536/1000. Epsilon: -4.370. Reward in last 100 episodes: 20.860\n",
            "Episode 538/1000. Epsilon: -4.390. Reward in last 100 episodes: 20.915\n",
            "Episode 540/1000. Epsilon: -4.410. Reward in last 100 episodes: 21.040\n",
            "Episode 542/1000. Epsilon: -4.430. Reward in last 100 episodes: 20.875\n",
            "Episode 544/1000. Epsilon: -4.450. Reward in last 100 episodes: 20.775\n",
            "Episode 546/1000. Epsilon: -4.470. Reward in last 100 episodes: 20.740\n",
            "Episode 548/1000. Epsilon: -4.490. Reward in last 100 episodes: 20.800\n",
            "Episode 550/1000. Epsilon: -4.510. Reward in last 100 episodes: 20.935\n",
            "Episode 552/1000. Epsilon: -4.530. Reward in last 100 episodes: 20.785\n",
            "Episode 554/1000. Epsilon: -4.550. Reward in last 100 episodes: 21.015\n",
            "Episode 556/1000. Epsilon: -4.570. Reward in last 100 episodes: 20.925\n",
            "Episode 558/1000. Epsilon: -4.590. Reward in last 100 episodes: 21.180\n",
            "Episode 560/1000. Epsilon: -4.610. Reward in last 100 episodes: 21.155\n",
            "Episode 562/1000. Epsilon: -4.630. Reward in last 100 episodes: 21.060\n",
            "Episode 564/1000. Epsilon: -4.650. Reward in last 100 episodes: 21.105\n",
            "Episode 566/1000. Epsilon: -4.670. Reward in last 100 episodes: 21.070\n",
            "Episode 568/1000. Epsilon: -4.690. Reward in last 100 episodes: 21.005\n",
            "Episode 570/1000. Epsilon: -4.710. Reward in last 100 episodes: 21.105\n",
            "Episode 572/1000. Epsilon: -4.730. Reward in last 100 episodes: 21.045\n",
            "Episode 574/1000. Epsilon: -4.750. Reward in last 100 episodes: 21.035\n",
            "Episode 576/1000. Epsilon: -4.770. Reward in last 100 episodes: 20.990\n",
            "Episode 578/1000. Epsilon: -4.790. Reward in last 100 episodes: 21.065\n",
            "Episode 580/1000. Epsilon: -4.810. Reward in last 100 episodes: 21.070\n",
            "Episode 582/1000. Epsilon: -4.830. Reward in last 100 episodes: 21.135\n",
            "Episode 584/1000. Epsilon: -4.850. Reward in last 100 episodes: 21.315\n",
            "Episode 586/1000. Epsilon: -4.870. Reward in last 100 episodes: 21.205\n",
            "Episode 588/1000. Epsilon: -4.890. Reward in last 100 episodes: 21.160\n",
            "Episode 590/1000. Epsilon: -4.910. Reward in last 100 episodes: 21.090\n",
            "Episode 592/1000. Epsilon: -4.930. Reward in last 100 episodes: 21.100\n",
            "Episode 594/1000. Epsilon: -4.950. Reward in last 100 episodes: 21.140\n",
            "Episode 596/1000. Epsilon: -4.970. Reward in last 100 episodes: 21.200\n",
            "Episode 598/1000. Epsilon: -4.990. Reward in last 100 episodes: 21.235\n",
            "Episode 600/1000. Epsilon: -5.010. Reward in last 100 episodes: 21.260\n",
            "Episode 602/1000. Epsilon: -5.030. Reward in last 100 episodes: 21.240\n",
            "Episode 604/1000. Epsilon: -5.050. Reward in last 100 episodes: 21.130\n",
            "Episode 606/1000. Epsilon: -5.070. Reward in last 100 episodes: 21.235\n",
            "Episode 608/1000. Epsilon: -5.090. Reward in last 100 episodes: 21.220\n",
            "Episode 610/1000. Epsilon: -5.110. Reward in last 100 episodes: 21.205\n",
            "Episode 612/1000. Epsilon: -5.130. Reward in last 100 episodes: 21.240\n",
            "Episode 614/1000. Epsilon: -5.150. Reward in last 100 episodes: 21.360\n",
            "Episode 616/1000. Epsilon: -5.170. Reward in last 100 episodes: 21.365\n",
            "Episode 618/1000. Epsilon: -5.190. Reward in last 100 episodes: 21.365\n",
            "Episode 620/1000. Epsilon: -5.210. Reward in last 100 episodes: 21.385\n",
            "Episode 622/1000. Epsilon: -5.230. Reward in last 100 episodes: 21.260\n",
            "Episode 624/1000. Epsilon: -5.250. Reward in last 100 episodes: 21.290\n",
            "Episode 626/1000. Epsilon: -5.270. Reward in last 100 episodes: 21.355\n",
            "Episode 628/1000. Epsilon: -5.290. Reward in last 100 episodes: 21.350\n",
            "Episode 630/1000. Epsilon: -5.310. Reward in last 100 episodes: 21.330\n",
            "Episode 632/1000. Epsilon: -5.330. Reward in last 100 episodes: 21.385\n",
            "Episode 634/1000. Epsilon: -5.350. Reward in last 100 episodes: 21.340\n",
            "Episode 636/1000. Epsilon: -5.370. Reward in last 100 episodes: 21.170\n",
            "Episode 638/1000. Epsilon: -5.390. Reward in last 100 episodes: 21.430\n",
            "Episode 640/1000. Epsilon: -5.410. Reward in last 100 episodes: 21.440\n",
            "Episode 642/1000. Epsilon: -5.430. Reward in last 100 episodes: 21.630\n",
            "Episode 644/1000. Epsilon: -5.450. Reward in last 100 episodes: 21.510\n",
            "Episode 646/1000. Epsilon: -5.470. Reward in last 100 episodes: 21.605\n",
            "Episode 648/1000. Epsilon: -5.490. Reward in last 100 episodes: 21.515\n",
            "Episode 650/1000. Epsilon: -5.510. Reward in last 100 episodes: 21.470\n",
            "Episode 652/1000. Epsilon: -5.530. Reward in last 100 episodes: 21.475\n",
            "Episode 654/1000. Epsilon: -5.550. Reward in last 100 episodes: 21.350\n",
            "Episode 656/1000. Epsilon: -5.570. Reward in last 100 episodes: 21.390\n",
            "Episode 658/1000. Epsilon: -5.590. Reward in last 100 episodes: 21.190\n",
            "Episode 660/1000. Epsilon: -5.610. Reward in last 100 episodes: 21.160\n",
            "Episode 662/1000. Epsilon: -5.630. Reward in last 100 episodes: 21.305\n",
            "Episode 664/1000. Epsilon: -5.650. Reward in last 100 episodes: 21.350\n",
            "Episode 666/1000. Epsilon: -5.670. Reward in last 100 episodes: 21.355\n",
            "Episode 668/1000. Epsilon: -5.690. Reward in last 100 episodes: 21.480\n",
            "Episode 670/1000. Epsilon: -5.710. Reward in last 100 episodes: 21.480\n",
            "Episode 672/1000. Epsilon: -5.730. Reward in last 100 episodes: 21.510\n",
            "Episode 674/1000. Epsilon: -5.750. Reward in last 100 episodes: 21.305\n",
            "Episode 676/1000. Epsilon: -5.770. Reward in last 100 episodes: 21.345\n",
            "Episode 678/1000. Epsilon: -5.790. Reward in last 100 episodes: 21.350\n",
            "Episode 680/1000. Epsilon: -5.810. Reward in last 100 episodes: 21.345\n",
            "Episode 682/1000. Epsilon: -5.830. Reward in last 100 episodes: 21.300\n",
            "Episode 684/1000. Epsilon: -5.850. Reward in last 100 episodes: 21.125\n",
            "Episode 686/1000. Epsilon: -5.870. Reward in last 100 episodes: 21.160\n",
            "Episode 688/1000. Epsilon: -5.890. Reward in last 100 episodes: 21.090\n",
            "Episode 690/1000. Epsilon: -5.910. Reward in last 100 episodes: 21.155\n",
            "Episode 692/1000. Epsilon: -5.930. Reward in last 100 episodes: 21.195\n",
            "Episode 694/1000. Epsilon: -5.950. Reward in last 100 episodes: 21.170\n",
            "Episode 696/1000. Epsilon: -5.970. Reward in last 100 episodes: 21.150\n",
            "Episode 698/1000. Epsilon: -5.990. Reward in last 100 episodes: 21.170\n",
            "Episode 700/1000. Epsilon: -6.010. Reward in last 100 episodes: 21.120\n",
            "Episode 702/1000. Epsilon: -6.030. Reward in last 100 episodes: 21.300\n",
            "Episode 704/1000. Epsilon: -6.050. Reward in last 100 episodes: 21.370\n",
            "Episode 706/1000. Epsilon: -6.070. Reward in last 100 episodes: 21.330\n",
            "Episode 708/1000. Epsilon: -6.090. Reward in last 100 episodes: 21.290\n",
            "Episode 710/1000. Epsilon: -6.110. Reward in last 100 episodes: 21.305\n",
            "Episode 712/1000. Epsilon: -6.130. Reward in last 100 episodes: 21.330\n",
            "Episode 714/1000. Epsilon: -6.150. Reward in last 100 episodes: 21.455\n",
            "Episode 716/1000. Epsilon: -6.170. Reward in last 100 episodes: 21.380\n",
            "Episode 718/1000. Epsilon: -6.190. Reward in last 100 episodes: 21.370\n",
            "Episode 720/1000. Epsilon: -6.210. Reward in last 100 episodes: 21.440\n",
            "Episode 722/1000. Epsilon: -6.230. Reward in last 100 episodes: 21.505\n",
            "Episode 724/1000. Epsilon: -6.250. Reward in last 100 episodes: 21.560\n",
            "Episode 726/1000. Epsilon: -6.270. Reward in last 100 episodes: 21.575\n",
            "Episode 728/1000. Epsilon: -6.290. Reward in last 100 episodes: 21.655\n",
            "Episode 730/1000. Epsilon: -6.310. Reward in last 100 episodes: 21.635\n",
            "Episode 732/1000. Epsilon: -6.330. Reward in last 100 episodes: 21.705\n",
            "Episode 734/1000. Epsilon: -6.350. Reward in last 100 episodes: 21.955\n",
            "Episode 736/1000. Epsilon: -6.370. Reward in last 100 episodes: 22.105\n",
            "Episode 738/1000. Epsilon: -6.390. Reward in last 100 episodes: 21.870\n",
            "Episode 740/1000. Epsilon: -6.410. Reward in last 100 episodes: 21.760\n",
            "Episode 742/1000. Epsilon: -6.430. Reward in last 100 episodes: 21.790\n",
            "Episode 744/1000. Epsilon: -6.450. Reward in last 100 episodes: 21.790\n",
            "Episode 746/1000. Epsilon: -6.470. Reward in last 100 episodes: 21.795\n",
            "Episode 748/1000. Epsilon: -6.490. Reward in last 100 episodes: 21.830\n",
            "Episode 750/1000. Epsilon: -6.510. Reward in last 100 episodes: 21.770\n",
            "Episode 752/1000. Epsilon: -6.530. Reward in last 100 episodes: 21.765\n",
            "Episode 754/1000. Epsilon: -6.550. Reward in last 100 episodes: 21.760\n",
            "Episode 756/1000. Epsilon: -6.570. Reward in last 100 episodes: 21.755\n",
            "Episode 758/1000. Epsilon: -6.590. Reward in last 100 episodes: 21.725\n",
            "Episode 760/1000. Epsilon: -6.610. Reward in last 100 episodes: 21.725\n",
            "Episode 762/1000. Epsilon: -6.630. Reward in last 100 episodes: 21.615\n",
            "Episode 764/1000. Epsilon: -6.650. Reward in last 100 episodes: 21.615\n",
            "Episode 766/1000. Epsilon: -6.670. Reward in last 100 episodes: 21.705\n",
            "Episode 768/1000. Epsilon: -6.690. Reward in last 100 episodes: 21.675\n",
            "Episode 770/1000. Epsilon: -6.710. Reward in last 100 episodes: 21.625\n",
            "Episode 772/1000. Epsilon: -6.730. Reward in last 100 episodes: 21.640\n",
            "Episode 774/1000. Epsilon: -6.750. Reward in last 100 episodes: 21.900\n",
            "Episode 776/1000. Epsilon: -6.770. Reward in last 100 episodes: 21.745\n",
            "Episode 778/1000. Epsilon: -6.790. Reward in last 100 episodes: 21.815\n",
            "Episode 780/1000. Epsilon: -6.810. Reward in last 100 episodes: 21.820\n",
            "Episode 782/1000. Epsilon: -6.830. Reward in last 100 episodes: 21.885\n",
            "Episode 784/1000. Epsilon: -6.850. Reward in last 100 episodes: 21.870\n",
            "Episode 786/1000. Epsilon: -6.870. Reward in last 100 episodes: 21.885\n",
            "Episode 788/1000. Epsilon: -6.890. Reward in last 100 episodes: 21.925\n",
            "Episode 790/1000. Epsilon: -6.910. Reward in last 100 episodes: 21.900\n",
            "Episode 792/1000. Epsilon: -6.930. Reward in last 100 episodes: 21.940\n",
            "Episode 794/1000. Epsilon: -6.950. Reward in last 100 episodes: 22.090\n",
            "Episode 796/1000. Epsilon: -6.970. Reward in last 100 episodes: 22.110\n",
            "Episode 798/1000. Epsilon: -6.990. Reward in last 100 episodes: 22.000\n",
            "Episode 800/1000. Epsilon: -7.010. Reward in last 100 episodes: 22.040\n",
            "Episode 802/1000. Epsilon: -7.030. Reward in last 100 episodes: 22.070\n",
            "Episode 804/1000. Epsilon: -7.050. Reward in last 100 episodes: 22.125\n",
            "Episode 806/1000. Epsilon: -7.070. Reward in last 100 episodes: 22.080\n",
            "Episode 808/1000. Epsilon: -7.090. Reward in last 100 episodes: 22.190\n",
            "Episode 810/1000. Epsilon: -7.110. Reward in last 100 episodes: 22.160\n",
            "Episode 812/1000. Epsilon: -7.130. Reward in last 100 episodes: 22.135\n",
            "Episode 814/1000. Epsilon: -7.150. Reward in last 100 episodes: 22.010\n",
            "Episode 816/1000. Epsilon: -7.170. Reward in last 100 episodes: 22.110\n",
            "Episode 818/1000. Epsilon: -7.190. Reward in last 100 episodes: 22.145\n",
            "Episode 820/1000. Epsilon: -7.210. Reward in last 100 episodes: 22.085\n",
            "Episode 822/1000. Epsilon: -7.230. Reward in last 100 episodes: 22.055\n",
            "Episode 824/1000. Epsilon: -7.250. Reward in last 100 episodes: 22.095\n",
            "Episode 826/1000. Epsilon: -7.270. Reward in last 100 episodes: 22.005\n",
            "Episode 828/1000. Epsilon: -7.290. Reward in last 100 episodes: 21.905\n",
            "Episode 830/1000. Epsilon: -7.310. Reward in last 100 episodes: 21.865\n",
            "Episode 832/1000. Epsilon: -7.330. Reward in last 100 episodes: 21.765\n",
            "Episode 834/1000. Epsilon: -7.350. Reward in last 100 episodes: 21.665\n",
            "Episode 836/1000. Epsilon: -7.370. Reward in last 100 episodes: 21.720\n",
            "Episode 838/1000. Epsilon: -7.390. Reward in last 100 episodes: 21.710\n",
            "Episode 840/1000. Epsilon: -7.410. Reward in last 100 episodes: 21.760\n",
            "Episode 842/1000. Epsilon: -7.430. Reward in last 100 episodes: 21.600\n",
            "Episode 844/1000. Epsilon: -7.450. Reward in last 100 episodes: 21.660\n",
            "Episode 846/1000. Epsilon: -7.470. Reward in last 100 episodes: 21.685\n",
            "Episode 848/1000. Epsilon: -7.490. Reward in last 100 episodes: 21.655\n",
            "Episode 850/1000. Epsilon: -7.510. Reward in last 100 episodes: 21.625\n",
            "Episode 852/1000. Epsilon: -7.530. Reward in last 100 episodes: 21.690\n",
            "Episode 854/1000. Epsilon: -7.550. Reward in last 100 episodes: 21.710\n",
            "Episode 856/1000. Epsilon: -7.570. Reward in last 100 episodes: 21.825\n",
            "Episode 858/1000. Epsilon: -7.590. Reward in last 100 episodes: 21.960\n",
            "Episode 860/1000. Epsilon: -7.610. Reward in last 100 episodes: 22.020\n",
            "Episode 862/1000. Epsilon: -7.630. Reward in last 100 episodes: 21.990\n",
            "Episode 864/1000. Epsilon: -7.650. Reward in last 100 episodes: 21.950\n",
            "Episode 866/1000. Epsilon: -7.670. Reward in last 100 episodes: 21.890\n",
            "Episode 868/1000. Epsilon: -7.690. Reward in last 100 episodes: 21.850\n",
            "Episode 870/1000. Epsilon: -7.710. Reward in last 100 episodes: 21.830\n",
            "Episode 872/1000. Epsilon: -7.730. Reward in last 100 episodes: 21.750\n",
            "Episode 874/1000. Epsilon: -7.750. Reward in last 100 episodes: 21.525\n",
            "Episode 876/1000. Epsilon: -7.770. Reward in last 100 episodes: 21.695\n",
            "Episode 878/1000. Epsilon: -7.790. Reward in last 100 episodes: 21.635\n",
            "Episode 880/1000. Epsilon: -7.810. Reward in last 100 episodes: 21.740\n",
            "Episode 882/1000. Epsilon: -7.830. Reward in last 100 episodes: 21.760\n",
            "Episode 884/1000. Epsilon: -7.850. Reward in last 100 episodes: 21.890\n",
            "Episode 886/1000. Epsilon: -7.870. Reward in last 100 episodes: 21.800\n",
            "Episode 888/1000. Epsilon: -7.890. Reward in last 100 episodes: 21.875\n",
            "Episode 890/1000. Epsilon: -7.910. Reward in last 100 episodes: 21.900\n",
            "Episode 892/1000. Epsilon: -7.930. Reward in last 100 episodes: 21.850\n",
            "Episode 894/1000. Epsilon: -7.950. Reward in last 100 episodes: 21.745\n",
            "Episode 896/1000. Epsilon: -7.970. Reward in last 100 episodes: 21.835\n",
            "Episode 898/1000. Epsilon: -7.990. Reward in last 100 episodes: 21.875\n",
            "Episode 900/1000. Epsilon: -8.010. Reward in last 100 episodes: 21.855\n",
            "Episode 902/1000. Epsilon: -8.030. Reward in last 100 episodes: 21.735\n",
            "Episode 904/1000. Epsilon: -8.050. Reward in last 100 episodes: 21.740\n",
            "Episode 906/1000. Epsilon: -8.070. Reward in last 100 episodes: 21.760\n",
            "Episode 908/1000. Epsilon: -8.090. Reward in last 100 episodes: 21.680\n",
            "Episode 910/1000. Epsilon: -8.110. Reward in last 100 episodes: 21.590\n",
            "Episode 912/1000. Epsilon: -8.130. Reward in last 100 episodes: 21.435\n",
            "Episode 914/1000. Epsilon: -8.150. Reward in last 100 episodes: 21.320\n",
            "Episode 916/1000. Epsilon: -8.170. Reward in last 100 episodes: 21.160\n",
            "Episode 918/1000. Epsilon: -8.190. Reward in last 100 episodes: 21.095\n",
            "Episode 920/1000. Epsilon: -8.210. Reward in last 100 episodes: 21.065\n",
            "Episode 922/1000. Epsilon: -8.230. Reward in last 100 episodes: 21.125\n",
            "Episode 924/1000. Epsilon: -8.250. Reward in last 100 episodes: 21.055\n",
            "Episode 926/1000. Epsilon: -8.270. Reward in last 100 episodes: 21.055\n",
            "Episode 928/1000. Epsilon: -8.290. Reward in last 100 episodes: 21.185\n",
            "Episode 930/1000. Epsilon: -8.310. Reward in last 100 episodes: 21.285\n",
            "Episode 932/1000. Epsilon: -8.330. Reward in last 100 episodes: 21.385\n",
            "Episode 934/1000. Epsilon: -8.350. Reward in last 100 episodes: 21.305\n",
            "Episode 936/1000. Epsilon: -8.370. Reward in last 100 episodes: 21.190\n",
            "Episode 938/1000. Epsilon: -8.390. Reward in last 100 episodes: 21.230\n",
            "Episode 940/1000. Epsilon: -8.410. Reward in last 100 episodes: 21.240\n",
            "Episode 942/1000. Epsilon: -8.430. Reward in last 100 episodes: 21.340\n",
            "Episode 944/1000. Epsilon: -8.450. Reward in last 100 episodes: 21.225\n",
            "Episode 946/1000. Epsilon: -8.470. Reward in last 100 episodes: 21.175\n",
            "Episode 948/1000. Epsilon: -8.490. Reward in last 100 episodes: 21.250\n",
            "Episode 950/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.445\n",
            "Episode 952/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.550\n",
            "Episode 954/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.620\n",
            "Episode 956/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.570\n",
            "Episode 958/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.455\n",
            "Episode 960/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.405\n",
            "Episode 962/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.435\n",
            "Episode 964/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.415\n",
            "Episode 966/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.365\n",
            "Episode 968/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.330\n",
            "Episode 970/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.365\n",
            "Episode 972/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.430\n",
            "Episode 974/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.560\n",
            "Episode 976/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.515\n",
            "Episode 978/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.470\n",
            "Episode 980/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.475\n",
            "Episode 982/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.395\n",
            "Episode 984/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.275\n",
            "Episode 986/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.310\n",
            "Episode 988/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.350\n",
            "Episode 990/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.425\n",
            "Episode 992/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.345\n",
            "Episode 994/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.330\n",
            "Episode 996/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.270\n",
            "Episode 998/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.300\n",
            "Episode 1000/1000. Epsilon: -8.500. Reward in last 100 episodes: 21.450\n"
          ]
        }
      ],
      "source": [
        "def select_epsilon_greedy_action(epsilon: float):\n",
        "    result = np.random.uniform(0, 1)\n",
        "    if result < epsilon:\n",
        "        # get random action from actions\n",
        "        action_ind = np.random.randint(0, len(actions))\n",
        "        debug(\"Selected action:\", actions[action_ind], \"(index\", str(action_ind) + \")\")\n",
        "        return action_ind\n",
        "    else:\n",
        "        # run all possible guesses through the model and select the best one\n",
        "        best_action_ind = None\n",
        "        best_q = -np.inf\n",
        "\n",
        "        # q = model_qp.predict([all possible ones])\n",
        "\n",
        "        all_actions = np.array(\n",
        "            [\n",
        "                np.concatenate((boardToState(wordleGame.board), actionIndToInput(i)))\n",
        "                for i in range(small_num_actions)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # return model_qp.predict(all_actions)\n",
        "        preds = model_qp.predict(all_actions, verbose=0)\n",
        "        # np.argmax(np.max(preds, axis=1))\n",
        "        return np.argmax(preds, axis=0)[0]\n",
        "\n",
        "\n",
        "buffer = ReplayBuffer(100000)\n",
        "cur_frame = 0\n",
        "\n",
        "last_100_ep_rewards = []\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "\n",
        "def train_step(states, actions, rewards, next_states, dones, potential_actions):\n",
        "    # length of states = batch_size * action_size\n",
        "\n",
        "    ## this is causing problems when the game resets...\n",
        "    q_prime_inputs = np.concatenate((next_states, potential_actions), axis=1)\n",
        "    q_primes = model_qp.predict(q_prime_inputs, verbose=0)\n",
        "\n",
        "    max_q_primes = tf.reduce_max(q_primes, axis=-1)\n",
        "    target = rewards + (1 - dones) * discount * np.max(q_primes, axis=1)\n",
        "\n",
        "    full_inputs = np.concatenate((states, potential_actions), axis=1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = model_q(full_inputs)\n",
        "        qs = tf.reduce_max(q_values, axis=-1)\n",
        "        loss = mse(target, tf.reduce_max(qs))\n",
        "    grads = tape.gradient(loss, model_q.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model_q.trainable_variables))\n",
        "\n",
        "\n",
        "for episode in range(num_episodes + 1):\n",
        "    state = reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "\n",
        "    ## play one game\n",
        "    while not done:\n",
        "        # curr_state = state\n",
        "        action = select_epsilon_greedy_action(epsilon)\n",
        "        next_state, reward, done, info = step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        ## save to buffer\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        cur_frame += 1\n",
        "\n",
        "        # copy Q weights to Q'\n",
        "        ## more often than medium because we have fewer episodes\n",
        "        if cur_frame % 200 == 0:\n",
        "            model_qp.set_weights(model_q.get_weights())\n",
        "\n",
        "        ## train neural network\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, taken_actions, rewards, next_states, dones, potential_actions = (\n",
        "                buffer.sample(batch_size, action_size)\n",
        "            )\n",
        "            loss = train_step(\n",
        "                states, actions, rewards, next_states, dones, potential_actions\n",
        "            )\n",
        "\n",
        "    if episode < 950:\n",
        "        epsilon -= 0.01\n",
        "\n",
        "    if len(last_100_ep_rewards) == 100:\n",
        "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
        "    last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "    if episode % 2 == 0:\n",
        "        print(\n",
        "            f\"Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. \"\n",
        "            f\"Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}\"\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
