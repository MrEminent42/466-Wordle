{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "BQBviblF-FmH"
      },
      "outputs": [],
      "source": [
        "def debug(*info):\n",
        "    p = False\n",
        "    if p:\n",
        "        print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhCoexnK7Cih"
      },
      "source": [
        "# Wordle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxto3T2_7HMU",
        "outputId": "8de180aa-06d4-4c81-ecf4-9be2d494ea18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: simple_colors in /usr/local/lib/python3.10/dist-packages (0.1.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install simple_colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "M8EBJOZz7CUT"
      },
      "outputs": [],
      "source": [
        "from enum import IntEnum\n",
        "from simple_colors import *\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "small_num_actions = 50\n",
        "\n",
        "actions = np.loadtxt(\"./actions.txt\", dtype=str)\n",
        "actions = random.sample([word.upper() for word in actions], small_num_actions)\n",
        "# actions = [word.upper() for word in actions]\n",
        "\n",
        "\n",
        "class Color(IntEnum):\n",
        "    GREY = GRAY = 0\n",
        "    YELLOW = 1\n",
        "    GREEN = 2\n",
        "\n",
        "\n",
        "class Tile:\n",
        "    def __init__(self, character: str, color: Color):\n",
        "        self.char = character\n",
        "        self.color = color\n",
        "\n",
        "\n",
        "class Board:\n",
        "    def __init__(self):\n",
        "        self.board = []\n",
        "\n",
        "    def append_row(self, row: list[Tile]):\n",
        "        self.board.append(row)\n",
        "\n",
        "    def get_row(self, i) -> list[Tile]:\n",
        "        return self.board[i]\n",
        "\n",
        "    def get_rows(self) -> list[list[Tile]]:\n",
        "        return self.board\n",
        "\n",
        "    def get_num_rows(self) -> int:\n",
        "        return len(self.board)\n",
        "\n",
        "\n",
        "class WordleGame:\n",
        "    def new_game(self, answer=None):\n",
        "        if answer == None:\n",
        "            self.answer = actions[np.random.randint(0, len(actions))]\n",
        "        else:\n",
        "            self.answer = answer\n",
        "        self.board = Board()\n",
        "        self.is_complete = False\n",
        "        self.win = False\n",
        "\n",
        "    def __init__(self, answer=None):\n",
        "        self.new_game(answer)\n",
        "\n",
        "    ## string representation of the Wordle board\n",
        "    ## returns with color too!\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        # in each line\n",
        "        for i, line in enumerate(self.board.get_rows()):\n",
        "            colors = [tile.color for tile in line]\n",
        "            for i, tile in enumerate(line):\n",
        "                # for i, char in enumerate(line):\n",
        "\n",
        "                if tile.color == Color.GREY:\n",
        "                    s += black(tile.char, \"bold\")\n",
        "                elif tile.color == Color.YELLOW:\n",
        "                    s += yellow(tile.char, \"bold\")\n",
        "                else:\n",
        "                    s += green(tile.char, \"bold\")\n",
        "            s += \"\\n\"\n",
        "        return s\n",
        "\n",
        "    ## Takes a five-letter guess, records this guess on the game's board.\n",
        "    ## Returns the array of Colors with each index corresponding to the color of the letter at that index in the guess\n",
        "    def guess(self, guess):\n",
        "        debug(\"Guessing\", guess, \"on board\")\n",
        "        debug(self)\n",
        "        tiles = []\n",
        "        if len(guess) != 5:\n",
        "            raise ValueError(\n",
        "                'Wordle guess must be a 5-letter word. Could not guess with word \"'\n",
        "                + guess\n",
        "                + '\".'\n",
        "            )\n",
        "        # convert everything to upper case\n",
        "        guess = guess.upper()\n",
        "        # debug print\n",
        "        debug(\"Your guess:\", guess)\n",
        "        debug(\"The answer:\", self.answer)\n",
        "        colors = self.get_colors(guess)\n",
        "        # log guess to board\n",
        "        tiles = [Tile(guess[i], colors[i]) for i in range(5)]\n",
        "        self.board.append_row(tiles)\n",
        "\n",
        "        # check for game over\n",
        "        if self.board.get_num_rows() >= 6:\n",
        "            self.is_complete = True\n",
        "        elif guess == self.answer:\n",
        "            debug(\"WIN!\")\n",
        "            self.is_complete = self.win = True\n",
        "\n",
        "        # give back list of colors\n",
        "        return colors\n",
        "\n",
        "    ## get the colors of a word guess\n",
        "    ## input: string\n",
        "    ## output: list of Colors, each color corresponding to the\n",
        "    ##         appropriate game color of the letter at that index\n",
        "    def get_colors(self, guess: str):\n",
        "        ## grey by default\n",
        "        colors = [Color.GREY for i in range(len(guess))]\n",
        "\n",
        "        # count # occurrences of each of the letters in the correct answer\n",
        "        occurrences_remaining = {}\n",
        "        for char in self.answer:\n",
        "            if char in occurrences_remaining:\n",
        "                occurrences_remaining[char] += 1\n",
        "            else:\n",
        "                occurrences_remaining[char] = 1\n",
        "\n",
        "        ## appropriately color the letters\n",
        "\n",
        "        ## greens first\n",
        "        ## if the character is in the correct place\n",
        "        for i, char in enumerate(guess):\n",
        "            if self.answer[i] == char:\n",
        "                colors[i] = Color.GREEN\n",
        "                occurrences_remaining[char] -= 1\n",
        "                debug(\"Green:\", char)\n",
        "\n",
        "        ## yellows next\n",
        "        ## if the character is in the word, but in the wrong place\n",
        "        for i, char in enumerate(guess):\n",
        "            ## skip if already colored greeen\n",
        "            ## skip if all occurrences of this letter have been accounted for\n",
        "            if (\n",
        "                colors[i] == Color.GREEN\n",
        "                or char not in occurrences_remaining\n",
        "                or occurrences_remaining[char] == 0\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            colors[i] = Color.YELLOW\n",
        "            debug(\"Yellow:\", char)\n",
        "            # record that we have accounted for this occurence\n",
        "            occurrences_remaining[char] -= 1\n",
        "\n",
        "        return colors\n",
        "\n",
        "    def is_complete(self):\n",
        "        return self.is_complete\n",
        "\n",
        "    def run_game(self):\n",
        "        print(\"Welcome to Wordle-AI!\")\n",
        "        while not self.is_complete:\n",
        "            self.guess(input(\"Guess: \"))\n",
        "            print(self)\n",
        "        if self.win:\n",
        "            print(\n",
        "                \"Congrats! You found the word in\", self.board.get_num_rows(), \"tries.\"\n",
        "            )\n",
        "        else:\n",
        "            print(\"Darn! You didn't find the word. It was \" + self.answer + \".\")\n",
        "\n",
        "    def is_win(self):\n",
        "        return self.win"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whIrU-fh7VIL"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "hW_GgsRp7kTN"
      },
      "outputs": [],
      "source": [
        "# create the environment\n",
        "wordleGame = WordleGame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Par1IMmC7d5Q"
      },
      "source": [
        "## wordle wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "VIgXlQbc7fPT"
      },
      "outputs": [],
      "source": [
        "## convert a row on a board to one-hot encoded format for alphabet letters\n",
        "def rowToOneHot(row: list[str]):\n",
        "    oneHot = np.zeros((5, 26))\n",
        "    for i, char in enumerate(row):\n",
        "        if char != None:\n",
        "            oneHot[i][ord(char) - ord(\"A\")] = 1\n",
        "    return oneHot\n",
        "\n",
        "\n",
        "## convert a board to a state, which is a flattened version of:\n",
        "## the board: (6 x 5) wordle board x 26 letters one-hot encoded\n",
        "## +  colors: (6 x 5) wordle board x 2 color layers (green and yellow,\n",
        "##                                                  grey is default)\n",
        "def boardToState(board: Board):\n",
        "    letters = np.zeros((6, 5, 26))\n",
        "    colors = np.zeros((6, 5, 2))\n",
        "    # 840 total size of board state\n",
        "\n",
        "    for i, row in enumerate(board.get_rows()):\n",
        "        for j, tile in enumerate(row):\n",
        "            letters[i][j][ord(tile.char) - ord(\"A\")] = 1\n",
        "            if tile.color == Color.GREEN:\n",
        "                colors[i][j][1] = 1\n",
        "            elif tile.color == Color.YELLOW:\n",
        "                colors[i][j][0] = 1\n",
        "\n",
        "    return np.concatenate((letters.flatten(), colors.flatten()))\n",
        "\n",
        "\n",
        "## get the reward of a guess based on the colors\n",
        "def getReward(colors: list[Color]):\n",
        "    reward = 0\n",
        "    win = True\n",
        "    for color in colors:\n",
        "        if color == Color.GREEN:\n",
        "            reward += 1.5\n",
        "        elif color == Color.YELLOW:\n",
        "            reward += 0.5\n",
        "            win = False\n",
        "        else:\n",
        "            reward += 0.2\n",
        "            win = False\n",
        "    if win:\n",
        "        reward += 16 - wordleGame.board.get_num_rows()\n",
        "    return reward\n",
        "\n",
        "\n",
        "## convert an action index to model input format\n",
        "def actionIndToInput(action_ind: int):\n",
        "    action = actions[action_ind]\n",
        "    word = []\n",
        "    for letter in action:\n",
        "        tile = [0 for i in range(26)]\n",
        "        tile[ord(letter) - ord(\"A\")] = 1\n",
        "        word.append(tile)\n",
        "    return np.concatenate(np.array(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "4heCIcq0QldL"
      },
      "outputs": [],
      "source": [
        "def inputToLegible(input):\n",
        "    # letters are first 6 x 5 x 26  = 780\n",
        "    arr = np.reshape(input[0:780], (6, 5, 26))\n",
        "    board = [[\"\" for i in range(5)] for i in range(6)]\n",
        "    for row in range(6):\n",
        "        for col in range(5):\n",
        "            for offset in range(26):\n",
        "                if arr[row][col][offset] == 1:\n",
        "                    board[row][col] = chr(offset + ord(\"A\"))\n",
        "        board[row] = \"\".join(board[row])\n",
        "\n",
        "    guess_arr = np.reshape(input[-130:], (5, 26))\n",
        "    word = [\"\" for i in range(5)]\n",
        "    for tile in range(5):\n",
        "        for offset in range(26):\n",
        "            if guess_arr[tile][offset] == 1:\n",
        "                word[tile] = chr(offset + ord(\"A\"))\n",
        "\n",
        "    return (board, \"\".join(word))\n",
        "\n",
        "\n",
        "# inputToLegible(np.concatenate((boardToState(wordleGame.board), actionIndToInput(1))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Lv2rl4yj7xFL"
      },
      "outputs": [],
      "source": [
        "## step function for the environment\n",
        "## input: int action for the word to guess\n",
        "## output: vector next_state, float reward, boolean done, None info\n",
        "def step(action: int):\n",
        "    ## verify action is legit\n",
        "    if action < 0 or action >= len(actions):\n",
        "        raise ValueError(\"Action out of bounds\")\n",
        "\n",
        "    ## take the action (guess)\n",
        "    guess = actions[action]\n",
        "    colors = wordleGame.guess(guess)\n",
        "    ## what we need: next_state, reward, done, info (not used)\n",
        "    reward = getReward(colors)\n",
        "    next_state = boardToState(wordleGame.board)\n",
        "    done = wordleGame.is_complete\n",
        "    return next_state, reward, done, None\n",
        "\n",
        "\n",
        "def reset():\n",
        "    debug(\"Resetting game\")\n",
        "    wordleGame.new_game(None)\n",
        "    return boardToState(wordleGame.board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeC046m7r3T"
      },
      "source": [
        "## replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "N0odSj0O72cn"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done: bool):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    ## random sample of the buffer crossed with a set of random actions of num_actions\n",
        "    ## input: int num_samples, int num_actions\n",
        "    ## output: tuple of np.arrays (states, actions, rewards, next_states, dones, potential_actions)\n",
        "    ## of length num_samples\n",
        "    def sample(self, num_samples):\n",
        "        states, selected_actions, rewards, next_states, dones, potential_actions = (\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "        )\n",
        "        indexes = np.random.choice(len(self.buffer), num_samples)\n",
        "\n",
        "        potential_action_indices = np.random.choice(len(actions), num_samples)\n",
        "\n",
        "        for sample_num in range(num_samples):\n",
        "            # for potential_action_index in potential_action_indices:\n",
        "            state, action, reward, next_state, done = self.buffer[sample_num]\n",
        "            states.append(state)\n",
        "            selected_actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(next_state)\n",
        "            dones.append(done)\n",
        "            potential_actions.append(\n",
        "                actionIndToInput(potential_action_indices[sample_num])\n",
        "            )\n",
        "\n",
        "        return (\n",
        "            np.array(states),  # vector array\n",
        "            np.array(actions),  # int array\n",
        "            np.array(rewards),  # float array\n",
        "            np.array(next_states),  # vector array\n",
        "            np.array(dones, dtype=float),  # vector array, 1.0 = true, 0.0 = false\n",
        "            np.array(potential_actions),  # int array\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsm4U9lb75E9"
      },
      "source": [
        "## models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "S7npGrnBUCls"
      },
      "outputs": [],
      "source": [
        "## model input size:\n",
        "## wordle letters:             6*5*26\n",
        "## colors for each tile:     + 6*5*2\n",
        "## action to predict reward: + 5*26\n",
        "##                           = 970"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Tj4DNoso8BOz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "enPGTHm37576"
      },
      "outputs": [],
      "source": [
        "model_policy = tf.keras.models.Sequential(\n",
        "    [\n",
        "        # tf.keras.layers.Flatten(input_shape=(840,)),  # 840 inputs\n",
        "        tf.keras.layers.Dense(970, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_target = tf.keras.models.Sequential(\n",
        "    [\n",
        "        # tf.keras.layers.Flatten(input_shape=(840,)),  # 840 inputs\n",
        "        tf.keras.layers.Dense(970, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3BGvLlG8Hby"
      },
      "source": [
        "## training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "UEJSFntfXd0_"
      },
      "outputs": [],
      "source": [
        "def get_model_input(state, action_ind):\n",
        "    return np.concatenate((boardToState(state), actionIndToInput(action_ind)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "4D8BEtAQm8iR"
      },
      "outputs": [],
      "source": [
        "# select_epsilon_greedy_action moved below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "gq0gb4g_8LLe"
      },
      "outputs": [],
      "source": [
        "num_episodes = 1000  # @param {type: \"integer\"}\n",
        "epsilon = 1.0  # @param {type: \"number\"}\n",
        "batch_size = 32  # @param {type: \"integer\"}\n",
        "# action_size = 32 # @param {type: \"integer\"}\n",
        "discount = 0.9  # @param {type: \"number\"}\n",
        "replay_size = 100000  # @param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPQtyL6-agZD"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iDxPjYbz0wqT",
        "outputId": "ca950ba7-31e5-40ea-ffb3-b444ca2108a5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yes'"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"yes\" if (np.array([0, 0]) == np.array([0, 0])).all() else \"no\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9P8deCH8hgw"
      },
      "outputs": [],
      "source": [
        "def select_epsilon_greedy_action(epsilon: float):\n",
        "    result = np.random.uniform(0, 1)\n",
        "    if result < epsilon:\n",
        "        # get random action from actions\n",
        "        action_ind = np.random.randint(0, len(actions))\n",
        "        debug(\"Selected action:\", actions[action_ind], \"(index\", str(action_ind) + \")\")\n",
        "        return action_ind\n",
        "    else:\n",
        "        # run all possible guesses through the model and select the best one\n",
        "        all_actions = np.array(\n",
        "            [\n",
        "                np.concatenate((boardToState(wordleGame.board), actionIndToInput(i)))\n",
        "                for i in range(small_num_actions)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # return model_target.predict(all_actions)\n",
        "        preds = model_target.predict(all_actions, verbose=0, use_multiprocessing=True)\n",
        "        # np.argmax(np.max(preds, axis=1))\n",
        "        return np.argmax(preds, axis=0)[0]\n",
        "\n",
        "\n",
        "buffer = ReplayBuffer(replay_size)\n",
        "cur_frame = 0\n",
        "\n",
        "last_100_ep_rewards = []\n",
        "last_100_ep_wins = []\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "\n",
        "def train_step(states, actions, rewards, next_states, dones, potential_actions):\n",
        "    # length of states/actions/etc. = batch_size parameter\n",
        "\n",
        "    q_prime_inputs = np.concatenate((next_states, potential_actions), axis=1)\n",
        "    q_primes = model_target.predict(q_prime_inputs, verbose=0)\n",
        "\n",
        "    max_q_primes = tf.reduce_max(q_primes, axis=-1)\n",
        "    target = rewards + (1 - dones) * discount * np.squeeze(q_primes, axis=1)\n",
        "\n",
        "    full_inputs = np.concatenate((states, potential_actions), axis=1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        ## predict the outcome of taking each action\n",
        "        q_values = model_policy(full_inputs)  ## shape: (batch_size, 1)\n",
        "        q_values = tf.squeeze(q_values, axis=-1)  ## shape: (batch_size, )\n",
        "\n",
        "        ## loss = target vs. maximum possible reward from taking this action\n",
        "        ##                   (out of all possible actions)\n",
        "        loss = mse(target, tf.reduce_max(q_values))  ## MSE of two numbers\n",
        "    grads = tape.gradient(loss, model_policy.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model_policy.trainable_variables))\n",
        "\n",
        "\n",
        "for episode in range(num_episodes + 1):\n",
        "    state = reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "\n",
        "    ## play one game\n",
        "    while not done:\n",
        "        ## select and perform an action\n",
        "        action = select_epsilon_greedy_action(epsilon)\n",
        "        next_state, reward, done, info = step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        ## save outcome to buffer\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        assert not (\n",
        "            (state == next_state).all()\n",
        "        )  ## confirm state is being updated properly\n",
        "        state = next_state\n",
        "        cur_frame += 1\n",
        "\n",
        "        # every so often, copy Q weights to Q'\n",
        "        # if cur_frame % 2000 == 0:\n",
        "        if cur_frame % 200 == 0:\n",
        "            model_target.set_weights(model_policy.get_weights())\n",
        "\n",
        "        ## train the Q neural network (policy network)\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, taken_actions, rewards, next_states, dones, potential_actions = (\n",
        "                buffer.sample(batch_size)\n",
        "            )\n",
        "            loss = train_step(\n",
        "                states, actions, rewards, next_states, dones, potential_actions\n",
        "            )\n",
        "\n",
        "    # if episode < 9500:\n",
        "    if episode < 950:\n",
        "        epsilon -= 0.001\n",
        "        # epsilon -= 0.01\n",
        "\n",
        "    if len(last_100_ep_rewards) == 100:\n",
        "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
        "        last_100_ep_wins = last_100_ep_wins[1:]\n",
        "    last_100_ep_rewards.append(ep_reward)\n",
        "    last_100_ep_wins.append(wordleGame.is_win())\n",
        "\n",
        "    if episode % 5 == 0:\n",
        "        print(\n",
        "            f\"Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. \"\n",
        "            f\"Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f} ({np.array(last_100_ep_wins).sum()} wins)\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPH1zOimoVK2"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz5tR6XqpKqy"
      },
      "outputs": [],
      "source": [
        "def select_greedy_action():\n",
        "    best_action_ind = None\n",
        "    best_q = -np.inf\n",
        "\n",
        "    all_actions = np.array(\n",
        "        [\n",
        "            np.concatenate((boardToState(wordleGame.board), actionIndToInput(i)))\n",
        "            for i in range(small_num_actions)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preds = model_qp.predict(all_actions, verbose=0)\n",
        "    return np.argmax(preds, axis=0)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkWTvDT-oWde"
      },
      "outputs": [],
      "source": [
        "## play 100 games and see the average number of guesses it takes\n",
        "\n",
        "for i in range(100):\n",
        "    state = reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = select_greedy_action(epsilon)\n",
        "        _, _, done, _ = step(action)\n",
        "\n",
        "    if wordleGame.is_complete:\n",
        "        print(\n",
        "            \"Completed game\",\n",
        "            i,\n",
        "            \"in\",\n",
        "            wordleGame.board.get_num_rows(),\n",
        "            \"guesses;\",\n",
        "            (\"WIN\" if wordleGame.is_win() else \"LOSE\"),\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
